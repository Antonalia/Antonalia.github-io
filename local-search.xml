<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Hexo+Fluid中Onblur事件，离开页面时的标题显示</title>
    <link href="/09Fluid-Onblur/"/>
    <url>/09Fluid-Onblur/</url>
    
    <content type="html"><![CDATA[<p>在博客配置下的hexo-blog_inject中，若是没有文件夹则新建，然后在其中新建<code>monitortext.ejs</code>,用于向页面中添加焦点监控代码。</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs css">&lt;% if(theme<span class="hljs-selector-class">.fun_features</span><span class="hljs-selector-class">.monitortext</span><span class="hljs-selector-class">.enable</span>) &#123; %&gt;<br>&lt;script type=&quot;<span class="hljs-selector-tag">text</span>/javascript&quot;&gt;<br>  <span class="hljs-comment">/*窗口监视*/</span><br>  <span class="hljs-selector-tag">var</span> originalTitle = document<span class="hljs-selector-class">.title</span>;<br>  window<span class="hljs-selector-class">.onblur</span> = function()&#123;document<span class="hljs-selector-class">.title</span> = &quot;&lt;%- theme<span class="hljs-selector-class">.fun_features</span><span class="hljs-selector-class">.monitortext</span><span class="hljs-selector-class">.text</span> %&gt;&quot;&#125;;<br>  window<span class="hljs-selector-class">.onfocus</span> = function()&#123;document<span class="hljs-selector-class">.title</span> = originalTitle&#125;;<br>&lt;/script&gt;<br>  &lt;% &#125; %&gt;<br></code></pre></td></tr></table></figure><p>然后在hexo-blog<code>page.js</code>文件，添加以下代码。也可以在官网找到具体描述<ahref="https://hexo.fluid-dev.com/docs/advance/#hexo-注入代码">进阶用法 |Hexo Fluid 用户手册 (fluid-dev.com)</a>。</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs js"><span class="hljs-comment">// 添加页面焦点监控文字</span><br>hexo.<span class="hljs-property">extend</span>.<span class="hljs-property">filter</span>.<span class="hljs-title function_">register</span>(<span class="hljs-string">&#x27;theme_inject&#x27;</span>, <span class="hljs-keyword">function</span>(<span class="hljs-params">injects</span>) &#123;<br>    <span class="hljs-comment">// 添加页面焦点监控文字(here)</span><br>    injects.<span class="hljs-property">bodyBegin</span>.<span class="hljs-title function_">file</span>(<span class="hljs-string">&#x27;monitortext&#x27;</span>, <span class="hljs-string">&quot;source/_inject/monitortext.ejs&quot;</span>);<br>  &#125;);<br></code></pre></td></tr></table></figure><p>最后编辑主题配置文件，即hexo-blog<code>_config.yml</code>文件，在<code>fun_features</code> 项下添加：</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs js"># 一些好玩的功能<br># <span class="hljs-title class_">Some</span> fun features<br><span class="hljs-attr">fun_features</span>:<br>  # 监控网页焦点，改变文字<br>  <span class="hljs-attr">monitortext</span>:<br>    <span class="hljs-attr">enable</span>: <span class="hljs-literal">true</span><br>    <span class="hljs-attr">text</span>: 你想添加的文字<br><br>  # 为 subtitle 添加打字机效果<br>  # <span class="hljs-title class_">Typing</span> animation <span class="hljs-keyword">for</span> subtitle<br>  <span class="hljs-attr">typing</span>:<br>    <span class="hljs-attr">enable</span>: <span class="hljs-literal">true</span><br><br></code></pre></td></tr></table></figure><h2 id="参考">参考</h2><p>1、https://alec-97.github.io/posts/3644508848/ 2、<ahref="https://hexo.fluid-dev.com/docs/advance/#hexo-注入代码">进阶用法 |Hexo Fluid 用户手册 (fluid-dev.com)</a></p>]]></content>
    
    
    <categories>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Fluid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hexo+Fluid中头像旋转呼吸发光的实现</title>
    <link href="/08avatar-rotation/"/>
    <url>/08avatar-rotation/</url>
    
    <content type="html"><![CDATA[<p>将以下代码复制到主题配置的hexo-blog_pages_about.styl中<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><code class="hljs css"><span class="hljs-selector-class">.about-avatar</span><br>  <span class="hljs-attribute">position</span> relative<br>  <span class="hljs-attribute">margin</span> -<span class="hljs-number">8rem</span> auto <span class="hljs-number">1rem</span><br>  <span class="hljs-attribute">width</span> <span class="hljs-number">10rem</span><br>  <span class="hljs-attribute">height</span> <span class="hljs-number">10rem</span><br>  <span class="hljs-attribute">z-index</span> <span class="hljs-number">3</span><br><br>  <span class="hljs-selector-tag">img</span><br>    <span class="hljs-attribute">width</span> <span class="hljs-number">100%</span><br>    <span class="hljs-attribute">height</span> <span class="hljs-number">100%</span><br>    <span class="hljs-attribute">border-radius</span> <span class="hljs-number">50%</span><br>    <span class="hljs-attribute">background-color</span> transparent<br>    <span class="hljs-attribute">object-fit</span> cover<br>    <span class="hljs-attribute">box-shadow</span> <span class="hljs-number">0</span> <span class="hljs-number">2px</span> <span class="hljs-number">5px</span> <span class="hljs-number">0</span> rgba(<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>, <span class="hljs-number">0.11</span>), <span class="hljs-number">0</span> <span class="hljs-number">2px</span> <span class="hljs-number">10px</span> <span class="hljs-number">0</span> rgba(<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>) // 头像框周围黑色包裹线条<br><br>    <span class="hljs-attribute">transition</span>: all <span class="hljs-number">1.7s</span><br>    <br>    animation: shadowBlink <span class="hljs-number">4s</span> infinite; // 一次闪烁的时间<br><span class="hljs-selector-class">.img-fluid</span><span class="hljs-selector-pseudo">:hover</span> &#123;<br>  <span class="hljs-attribute">background-color</span>: <span class="hljs-number">#00FFFF</span>;<br>  -webkit-<span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">0px</span> <span class="hljs-number">0px</span> <span class="hljs-number">10px</span> <span class="hljs-number">10px</span> <span class="hljs-built_in">rgba</span>(<span class="hljs-number">255</span>,<span class="hljs-number">255</span>,<span class="hljs-number">255</span>, <span class="hljs-number">0.7</span>);<br>  // <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">0px</span> <span class="hljs-number">0px</span> <span class="hljs-number">70px</span> <span class="hljs-number">6px</span> <span class="hljs-built_in">rgba</span>(<span class="hljs-number">230</span>, <span class="hljs-number">230</span>, <span class="hljs-number">90</span>, <span class="hljs-number">1</span>); // 旋转时的发光阴影，分别是水平偏移、垂直偏移、模糊半径（羽化颜色）、阴影尺寸(实心颜色)和RGBA颜色<br><br>     <span class="hljs-attribute">transform</span>: <span class="hljs-built_in">rotate</span>(<span class="hljs-number">360deg</span>);<br>     -webkit-<span class="hljs-attribute">transform</span>: <span class="hljs-built_in">rotate</span>(<span class="hljs-number">360deg</span>);<br>     -moz-<span class="hljs-attribute">transform</span>: <span class="hljs-built_in">rotate</span>(<span class="hljs-number">360deg</span>);<br>     -o-<span class="hljs-attribute">transform</span>: <span class="hljs-built_in">rotate</span>(<span class="hljs-number">360deg</span>);<br>     -ms-<span class="hljs-attribute">transform</span>: <span class="hljs-built_in">rotate</span>(<span class="hljs-number">360deg</span>);<br>&#125;<br><span class="hljs-keyword">@keyframes</span> shadowBlink &#123;<br>  <span class="hljs-number">0%</span> &#123;<br>    <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">0px</span> <span class="hljs-number">0px</span> <span class="hljs-number">70px</span> <span class="hljs-number">6px</span> <span class="hljs-built_in">rgba</span>(<span class="hljs-number">230</span>, <span class="hljs-number">230</span>, <span class="hljs-number">90</span>, <span class="hljs-number">1</span>); <span class="hljs-comment">/* 初始阴影 */</span><br>  &#125;<br>  <span class="hljs-number">50%</span> &#123;<br>    <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">0px</span> <span class="hljs-number">0px</span> <span class="hljs-number">70px</span> <span class="hljs-number">6px</span> <span class="hljs-built_in">rgba</span>(<span class="hljs-number">230</span>, <span class="hljs-number">230</span>, <span class="hljs-number">90</span>, <span class="hljs-number">0.3</span>); <span class="hljs-comment">/* 闪烁时的半透明阴影 */</span><br>  &#125;<br>  <span class="hljs-number">100%</span> &#123;<br>    <span class="hljs-attribute">box-shadow</span>: <span class="hljs-number">0px</span> <span class="hljs-number">0px</span> <span class="hljs-number">70px</span> <span class="hljs-number">6px</span> <span class="hljs-built_in">rgba</span>(<span class="hljs-number">230</span>, <span class="hljs-number">230</span>, <span class="hljs-number">90</span>, <span class="hljs-number">1</span>); <span class="hljs-comment">/* 回到初始阴影 */</span><br>  &#125;<br>&#125;<br><br><span class="hljs-selector-class">.about-info</span><br>  &amp; &gt; <span class="hljs-selector-tag">div</span><br>    <span class="hljs-attribute">margin-bottom</span> .<span class="hljs-number">5rem</span><br><br><span class="hljs-selector-class">.about-name</span><br>  <span class="hljs-attribute">font-size</span> <span class="hljs-number">1.75rem</span><br>  <span class="hljs-attribute">font-weight</span> bold<br><br><span class="hljs-selector-class">.about-intro</span><br>  <span class="hljs-attribute">font-size</span> <span class="hljs-number">1rem</span><br><br><span class="hljs-selector-class">.about-icons</span><br>  &amp; &gt; <span class="hljs-selector-tag">a</span><span class="hljs-selector-pseudo">:not</span>(<span class="hljs-selector-pseudo">:last-child</span>)<br>    <span class="hljs-attribute">margin-right</span> .<span class="hljs-number">5rem</span><br><br>  &amp; &gt; <span class="hljs-selector-tag">a</span> &gt; <span class="hljs-selector-tag">i</span><br>    <span class="hljs-attribute">font-size</span> <span class="hljs-number">1.5rem</span><br><br></code></pre></td></tr></table></figure></p><h2 id="参考">参考</h2><p>1、<ahref="https://www.zzzwb.com/2023/09-27-css002.html#:~:text=fluid主题设置关">fluid主题设置关于页头像图片鼠标悬停360°旋转效果- Wenbin's blog (zzzwb.com)</a></p>]]></content>
    
    
    <categories>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Fluid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python中super()函数的应用</title>
    <link href="/07super/"/>
    <url>/07super/</url>
    
    <content type="html"><![CDATA[<h2 id="类的分类经典类和新式类">类的分类:经典类和新式类</h2><p>在py2中有经典类和新式类的区别：新式类：继承了object类的子类，以及该子类的子类，子子类经典类：没有继承object类的子类，以及该子类的子类，子子类 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment">#python2中</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Father</span>:<br>    <span class="hljs-keyword">pass</span>     <span class="hljs-comment"># 经典类</span><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Father</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">pass</span>     <span class="hljs-comment"># 新式类</span><br></code></pre></td></tr></table></figure>==注意：在py3中没有继承任何类，默认继承object类，所以python3中<strong>都是新式类</strong>==</p><h2 id="子类调用父类的三种方法">子类调用父类的三种方法</h2><ul><li>父类名.方法名(self)</li><li>super(子类名，self).父类方法名()</li><li>super().父类方法名</li></ul><h2 id="多类继承-菱形结构">多类继承-菱形结构</h2><p>若子类继承于多个类，如Child继承于Mother和Father，Mother和Father继承于object类时就会组成一个菱形关系。若子类调用的方法在父类中重名时，根据mro列表的顺序先输出mro列表左边类的方法。<code>类名.__base__</code>：查找类的父类 <code>类名.__mro__</code>：查找类的继承顺序 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Mother</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">speak</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;I&#x27;m your mother!&quot;</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Father</span>(<span class="hljs-title class_ inherited__">object</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">speak</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;I&#x27;m your father!&quot;</span>)<br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Child</span>(Mother, Father):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">say</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-comment"># 方式一</span><br>        <span class="hljs-comment"># (单和多继承 都适用)</span><br>        Father.speak(<span class="hljs-variable language_">self</span>)<br><br>        <span class="hljs-comment"># 方式二</span><br>        <span class="hljs-comment"># super默认会调用第一个父类的方法(适用于单继承 或者只想使用第一个父类的方法)</span><br>        <span class="hljs-comment"># 格式: super(子类类名, self).父类方法名()</span><br>        <span class="hljs-built_in">super</span>(Child, <span class="hljs-variable language_">self</span>).speak()<br><br>        <span class="hljs-comment"># 方式三 </span><br>        <span class="hljs-comment"># (适用于新式类) 02方式的简写</span><br>        <span class="hljs-built_in">super</span>().speak()<br><br>I = Child() <span class="hljs-comment"># 实例化</span><br><span class="hljs-built_in">print</span>(Child.__base__) <span class="hljs-comment"># 查找Child的父类</span><br><span class="hljs-built_in">print</span>(Child.__mro__) <span class="hljs-comment"># Child的继承顺序</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;----------------&quot;</span>)<br>I.say()<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;----------------&quot;</span>)<br>I.speak() <span class="hljs-comment"># 根据Child类在mro列表中的位置可知父类是Mother</span><br></code></pre></td></tr></table></figure> 输出结果： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;__main__.Mother&#x27;</span>&gt;<br>(&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;__main__.Child&#x27;</span>&gt;, &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;__main__.Mother&#x27;</span>&gt;, &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;__main__.Father&#x27;</span>&gt;, &lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;object&#x27;</span>&gt;)<br>----------------<br>I<span class="hljs-string">&#x27;m your father!</span><br><span class="hljs-string">I&#x27;</span>m your mother!<br>I<span class="hljs-string">&#x27;m your mother!</span><br><span class="hljs-string">----------------</span><br><span class="hljs-string">I&#x27;</span>m your mother!<br></code></pre></td></tr></table></figure></p><h2 id="参考">参考</h2><p>1、<ahref="https://blog.csdn.net/pythondby/article/details/121758141">python继承（史上最详细版本）</a></p>]]></content>
    
    
    <categories>
      
      <category>Python学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>python语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python中@staticmethod和@classmethod在函数中的应用</title>
    <link href="/06staticmethod-classmethod-function/"/>
    <url>/06staticmethod-classmethod-function/</url>
    
    <content type="html"><![CDATA[<h2 id="staticmethod静态方法"><span class="citation"data-cites="staticmethod">@staticmethod</span>：静态方法</h2><p>该方法不需要访问任何实例方法和属性，纯粹地传入参数并返回数据。它节省了实例化对象的开销成本，往往这种方法放在类外面的模块层作为一个函数存在也是没问题的，而放在类中，仅为这个类服务。如下声明一个静态方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">A</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @staticmethod</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">f</span>(<span class="hljs-params">arg1, arg2, ...</span>):<br>        ...<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">f1</span>(<span class="hljs-params">s</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Hello! &#x27;</span> + s)<br>        <br>A.f1(<span class="hljs-string">&#x27;Sam&#x27;</span>) <span class="hljs-comment"># 不实例化使用方法</span><br><br>a = A() <span class="hljs-comment"># 实例化使用方法</span><br>a.f1(<span class="hljs-string">&#x27;John&#x27;</span>)<br></code></pre></td></tr></table></figure><p>输出结果： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Hello! Sam<br>Hello! John<br></code></pre></td></tr></table></figure>以上实例声明了静态方法f1，从而可以实现==实例化调用==<code>A().f1()</code>，当然也可以==不实例化调用==该方法<code>A.f1()</code>。</p><h2 id="classmethod类方法"><span class="citation"data-cites="classmethod">@classmethod</span>：类方法</h2><p>第一个参数必须是<code>cls</code> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">A</span>(<span class="hljs-title class_ inherited__">object</span>):<br><span class="hljs-meta">    @classmethod</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">f2</span>(<span class="hljs-params">cls,s</span>):<br>        <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Hello! &#x27;</span> + s)<br>a = A()<br>a.f2(<span class="hljs-string">&#x27;Sam&#x27;</span>)<br>A.f2(<span class="hljs-string">&#x27;John&#x27;</span>)<br></code></pre></td></tr></table></figure> 输出结果：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">Hello! Sam<br>Hello! John<br></code></pre></td></tr></table></figure>以上结果表明，不管是<code>a.f2</code>还是<code>A.f2</code>，都是对应的A类，绑定了类对象中的f2方法。因为python可以通过实例对象a找到它所属的类是A，找到A之后自动绑定到<code>cls</code>。因此也可以在实例方法中通过<code>self.f2()</code>来调用类方法。<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">f0</span>(<span class="hljs-params">self, s</span>):<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;self:&quot;</span>, <span class="hljs-variable language_">self</span>)<br>    <span class="hljs-variable language_">self</span>.f2(s)<br></code></pre></td></tr></table></figure> ## 参考</p><p>1、<a href="https://zhuanlan.zhihu.com/p/28010894">正确理解Python中的<span class="citation"data-cites="staticmethod">@staticmethod</span><span class="citation"data-cites="classmethod方法">@classmethod方法</span></a></p>]]></content>
    
    
    <categories>
      
      <category>Python学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>python语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Python中property和@property的应用</title>
    <link href="/05property-function/"/>
    <url>/05property-function/</url>
    
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p><code>property</code>把方法包装成属性，让那个方法以属性的形式被访问和调用，即将<code>class.function()</code>通过<code>class.function</code>进行引用，不需要()</p><h2 id="property函数">1、property()函数</h2><ul><li>语法<code>property(fget=None, fset=None, fdel=None, doc=None) -&gt; property attribute</code></li><li>说明<ul><li><code>fget</code> 是获取属性值的方法</li><li><code>fset</code> 是设置属性值的方法</li><li><code>fdel</code> 是删除属性值的方法</li><li><code>doc</code> 是属性描述信息。如果省略，会把 fget 方法的docstring 拿来用（如果有的话）</li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Student</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>): <br>        <span class="hljs-variable language_">self</span>._age = <span class="hljs-literal">None</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">get_age</span>(<span class="hljs-params">self</span>): <span class="hljs-comment"># 获取属性时执行的代码</span><br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._age<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">set_age</span>(<span class="hljs-params">self, age</span>): <span class="hljs-comment"># 设置属性时执行的代码</span><br>        <span class="hljs-variable language_">self</span>._age = age<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">del_age</span>(<span class="hljs-params">self</span>): <span class="hljs-comment"># 删除属性时执行的代码</span><br>        <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>._age<br><br>    age = <span class="hljs-built_in">property</span>(get_age, set_age, del_age, <span class="hljs-string">&#x27;学生年龄&#x27;</span>)<br><br>student = Student()<br><span class="hljs-comment"># 用类名.属性.__doc__ 的形式查看属性的文档字符串,即property中的字符串注释</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;查看属性的文档字符串：&#x27;</span> + Student.age.__doc__)<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出：</span><br><span class="hljs-string">查看属性的文档字符串：学生年龄</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment"># 设置属性</span><br>student.age = <span class="hljs-number">18</span><br><br><span class="hljs-comment"># 获取属性</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;学生年龄为：&#x27;</span> + <span class="hljs-built_in">str</span>(student.age))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出：</span><br><span class="hljs-string">学生年龄为：18</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-comment"># 删除属性</span><br><span class="hljs-keyword">del</span> student.age<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;学生年龄为：&#x27;</span> + <span class="hljs-built_in">str</span>(student.age))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出：</span><br><span class="hljs-string">AttributeError: &#x27;Student&#x27; object has no attribute &#x27;_age&#x27;</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure><h2 id="property装饰器">2、<span class="citation"data-cites="property装饰器">@property装饰器</span></h2><p>这是更为常用的方法 - 被<code>@property</code>装饰的方法会被用做==属性名==。该条使用最为广泛。法加入@property后，这个方法相当于一个属性，这个属性可以让用户进行使用，并且在不设置<code>@属性名.setter</code>和<code>@属性名.deleter</code>时用户==没有办法随意修改==。 - 被 <code>@属性名.setter</code>装饰的方法是设置属性值的方法。 - 被 <code>@属性名.deleter</code>装饰的方法是删除属性值的方法 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Student</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>._age = <span class="hljs-literal">None</span><br><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">age</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._age<br><br><span class="hljs-meta">    @age.setter</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">age</span>(<span class="hljs-params">self, age</span>):<br>        <span class="hljs-variable language_">self</span>._age = age<br><br><span class="hljs-meta">    @age.deleter</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">age</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">del</span> <span class="hljs-variable language_">self</span>._age<br><br><br>student = Student()<br><br><span class="hljs-comment"># 设置属性</span><br>student.age = <span class="hljs-number">18</span><br><br><span class="hljs-comment"># 获取属性，即调用时不需要student.age()</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;学生年龄为：&#x27;</span> + <span class="hljs-built_in">str</span>(student.age))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出</span><br><span class="hljs-string">学生年龄为：18</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure>更一般的情况是：不设置<code>@属性名.setter</code> 和<code>@属性名.deleter</code>，则用户进行属性调用的时候，直接调用age即可。同时用户不知道属性名_age，因此用户无法更改属性，从而保护了类的属性<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Student</span>:<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-variable language_">self</span>._age = <span class="hljs-number">20</span><br><span class="hljs-meta">    @property</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">age</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>._age<br><br><br>student = Student()<br><br><span class="hljs-comment"># 设置属性</span><br>student.age = <span class="hljs-number">18</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><span class="hljs-string">报错：</span><br><span class="hljs-string">property &#x27;age&#x27; of &#x27;Student&#x27; object has no setter</span><br><span class="hljs-string">&#x27;&#x27;&#x27;</span><br><br><span class="hljs-comment"># 获取属性，</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;学生年龄为：&#x27;</span> + <span class="hljs-built_in">str</span>(student.age))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出</span><br><span class="hljs-string">学生年龄为：20</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br><span class="hljs-comment">#只有通过内部属性才能更改值，更加安全</span><br>student._age = <span class="hljs-number">18</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;学生年龄为：&#x27;</span> + <span class="hljs-built_in">str</span>(student.age))<br><span class="hljs-string">&quot;&quot;&quot;</span><br><span class="hljs-string">输出</span><br><span class="hljs-string">学生年龄为：18</span><br><span class="hljs-string">&quot;&quot;&quot;</span><br><br></code></pre></td></tr></table></figure></p><h2 id="参考">参考</h2><p>1、<ahref="https://blog.csdn.net/jpch89/article/details/84026130">Python 中property() 函数及 <span class="citation"data-cites="property">@property</span> 装饰器的使用_python中的property-CSDN博客</a> 2、<ahref="https://zhuanlan.zhihu.com/p/64487092">python <spanclass="citation"data-cites="property的介绍与使用">@property的介绍与使用</span> - 知乎(zhihu.com)</a></p>]]></content>
    
    
    <categories>
      
      <category>Python学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>python</tag>
      
      <tag>python语法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>SAC算法</title>
    <link href="/04SAC/"/>
    <url>/04SAC/</url>
    
    <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>不过，SAC本身介绍的能量模型和最大熵模型内容已经能够形成一条完备的逻辑链了。从基本模型开始，看看SAC这一条路径到底干了些什么吧~</p><h2 id="最大熵模型">1、最大熵模型</h2><h3 id="为什么要用最大熵模型">1.1 为什么要用最大熵模型</h3><p>传统的强化学习的目的是找出一个策略来最大化奖励，即目标函数：</p><p><span class="math display">\[π^*= \arg \max _{\pi}\mathbb {E}_{(st,at)∼π}[\Sigma_tR(s_t,a_t)]\]</span> 而最大熵强化学习是在最大化奖励的同时最大化熵： <spanclass="math display">\[π^*= \arg \max _{\pi}\mathbb {E}_{(st,at)∼π}[\Sigma_tR(s_t,a_t)+\alphaH(\pi(·|s_t))]\]</span></p><blockquote><p>信息熵的公式为：<spanclass="math inline">\(H(X)=−\sum_{i=1}^np(x_i)log⁡p(x_i)\)</span>用来表示</p></blockquote><p>这样子做有几个好处：</p><ul><li><strong>加强探索</strong>。利用熵让每一步的决策增加随机性，相当于强化了RL中的探索。这一方面能够加速之后的学习，另一方面可以防止策略过早地收敛到局部最优解。</li><li><strong>变确定性策略为随机策略</strong>。学过博弈论的小伙伴可能知道，在进行博弈时一个人的最优策略往往是混合策略，即随机策略往往是最优策略。增加熵可以增强策略的随机性，从而有助于收敛到最优策略。</li><li><strong>作为复杂任务的初始化</strong>。最大熵策略不仅学到一个策略，还能够捕获到所有的行为模式，这类似于CV、NLP中预训练的想法，先训练一个捕获到很多模式的模型，再微调参数应用到自己的模型，这大大加快了训练的速度。</li><li><strong>加强<ahref="https://zhida.zhihu.com/search?q=%E9%B2%81%E6%A3%92%E6%80%A7&amp;zhida_source=entity&amp;is_preview=1">鲁棒性</a></strong>。仅仅最大化奖励的话，策略倾向于只找到一条路径，如果这条路径短了，那策略的表现就会迅速下降；而最大熵策略可以找到所有的行为模式，就算一条路径断了，还可以从另一条路径达到同样的结果。</li></ul><h3 id="sac的最大熵模型的不同之处">1.2 SAC的最大熵模型的不同之处</h3><p>其实，在SAC之前，很多强化学习算法都用到了熵作为正则化参数来增强策略的探索。</p><p>A3C：</p><p>∇θ′log⁡π(at∣st;θ′)(Rt−V(st;θv))+β∇θ′H(π(st;θ′))</p><p>PPO：</p><p>LtCLIP+VF+S(θ)=E^t[LtCLIP(θ)−c1LtVF(θ)+c2H[πθ](st)]</p><p>PGQL：</p><p>Δθ∝Es,aQπ(s,a)∇θlog⁡π(s,a)+αE∇θHπ(s)s</p><p> class Student:    def <strong>init</strong>(self):        self._age= 20    <span class="citation" data-cites="property">@property</span>   def age(self):        return self._age​​student = Student()​#设置属性student.age = 18'''报错：property 'age' of 'Student' object hasno setter'''​# 获取属性，print('学生年龄为：' +str(student.age))"""输出学生年龄为：20"""​#只有通过内部属性才能更改值，更加安全student._age= 18print('学生年龄为：' +str(student.age))"""输出学生年龄为：18"""​python</p><blockquote><p>这个地方也有人认为SAC的熵经过策略<ahref="https://zhida.zhihu.com/search?q=%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86&amp;zhida_source=entity&amp;is_preview=1">梯度定理</a>推导之后等价于A3C的熵，不过无论如何，SAC的文章真正从理论上分析了VQπ中熵的存在性和可行性</p></blockquote><h3 id="从贝尔曼方程理解最大熵模型的值函数">1.3 从<ahref="https://zhida.zhihu.com/search?q=%E8%B4%9D%E5%B0%94%E6%9B%BC%E6%96%B9%E7%A8%8B&amp;zhida_source=entity&amp;is_preview=1">贝尔曼方程</a>理解最大熵模型的值函数</h3><blockquote><p>借用了下<ahref="https://zhuanlan.zhihu.com/p/70360272">这个</a>画的图</p></blockquote><p><strong>1）</strong>首先看看普通强化学习的目标函数： <spanclass="math display">\[\pi^{*}=\arg \max _{\pi} \mathbb{E}_{\left(s_{t}, a_{t}\right) \sim\rho_{\pi}}\left[\sum_{t} R\left(s_{t}, a_{t}\right)\right]\]</span></p><p>其贝尔曼方程为：</p><p><span class="math display">\[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \leftarrowr\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \sim p, \mathbf{a}_{t+1} \sim\pi}\left[Q\left(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}\right)\right]\]</span></p><p><imgsrc="https://pic1.zhimg.com/v2-c6f4a72b394358378732c312ab1e19a2_b.jpg" /></p><p>回溯图</p><p>从回溯图的角度，容易看到树的根结点<spanclass="math inline">\(q_{\pi}(s, a)\)</span> 实际上就是<spanclass="math inline">\(r\)</span>与 $q_{}(s^{}, a^{}) $的期望的和。</p><p><strong>2）</strong>再看看最大熵强化学习的目标函数： <spanclass="math display">\[\pi^{*}=\arg \max _{\pi} \mathbb{E}_{\left(s_{t}, a_{t}\right) \simp_{\pi}}[\sum_{t} \underbrace{R\left(s_{t}, a_{t}\right)}_{\text {reward}}+\alpha \underbrace{H\left(\pi\left(\cdot \mids_{t}\right)\right)}_{\text {entropy }}]\]</span></p><blockquote><p>这个 <span class="math inline">\(\alpha\)</span>是温度系数，用来衡量熵项的重要程度，后面的推导中如果没有写这一项的话是因为默认值为1。</p></blockquote><p>可以看到这个熵是加在<strong>状态</strong>上的，和动作无关。反映到回溯图中，就是：</p><p><imgsrc="https://pic3.zhimg.com/v2-adba9e16e4c9c8ea31635a9a1abd3946_b.jpg" /></p><p>加了熵的回溯图</p><p>类比一下，可以把$H ((|s^)) <spanclass="math inline">\(看作是状态价值函数，把\)</span>(a_{t+1}|s_{t+1})<span class="math inline">\(看作动作价值函数，那么：\)</span>$ H((|s_{t+1}))=-<em>{a</em>{t+1} A}p(a_{t+1}|s_{t+1})(a_{t+1}|s_{t+1})=-<em>{a</em>{t+1} }(a_{t+1}|s_{t+1} ) $$即在一个状态下的熵=在这个状态下对于每一个动作的可能性进行求期望，再反映到回溯图中，就可以看到熵和q是可以写在一起的：</p><p><imgsrc="https://pica.zhimg.com/v2-47d34a96b5f91423bad44b7b3b2c5cb6_b.jpg" /></p><p>熵拆解后的回溯图</p><p>于是贝尔曼方程可以写为：</p><p><span class="math display">\[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \leftarrowr\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \sim p, \mathbf{a}_{t+1} \sim\pi}\left[Q\left(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}\right)-\log\pi\left(\mathbf{a}_{t+1} \mid \mathbf{s}_{t+1}\right)\right]\]</span> 而我们知道在之前的强化学习中，贝尔曼方程中Q与V的关系是：</p><p><span class="math display">\[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \triangleqr\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \simp}\left[V\left(\mathbf{s}_{t+1}\right)\right]\]</span> <spanclass="math inline">\(p=p(s_{t+1}|s_t)\)</span>即从状态<spanclass="math inline">\(s_{t}\)</span>转移到<spanclass="math inline">\(s_{t+1}\)</span>的状态转移分布或概率函数，就是状态转移的概率。</p><p>因此，对比以上两式，就可以定义<spanclass="math inline">\(V\)</span>函数：</p><p><span class="math display">\[r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \sim p, \mathbf{a}_{t+1} \sim\pi}\left[Q\left(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}\right)-\log\pi\left(\mathbf{a}_{t+1} \mid\mathbf{s}_{t+1}\right)\right]=r\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \simp}\left[V\left(\mathbf{s}_{t+1}\right)\right]\\\gamma \mathbb{E}_{\mathbf{s}_{t+1} \sim p, \mathbf{a}_{t+1} \sim\pi}\left[Q\left(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}\right)-\log\pi\left(\mathbf{a}_{t+1} \mid \mathbf{s}_{t+1}\right)\right]=\gamma\mathbb{E}_{\mathbf{s}_{t+1} \simp}\left[V\left(\mathbf{s}_{t+1}\right)\right]\\V\left(\mathbf{s}_{t}\right)=\mathbb{E}_{\mathbf{a}_{t} \sim\pi}\left[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right]\]</span></p><h3 id="直接从公式理解值函数">1.4 直接从公式理解值函数</h3><p>1.3节从贝尔曼方程的角度理解了值函数之间的关系，不过说到底还是图像化的理解，从公式的角度出发其实可以直接定义出来<spanclass="math inline">\(Q、V\)</span>，就是没1.3那么好理解而已。不过从严谨性的角度，公式化的定义是非常有必要的。</p><p>1）首先看看标准的<spanclass="math inline">\(Q、V\)</span>函数，其被定义为条件化的累积奖励：<span class="math display">\[Q_{\pi}(s, a)=\underset{s_{t}, a_{t} \sim\pi}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t} r\left(s_{t},a_{t}\right) \mid s_{0}=s, a_{0}=a\right]\\V_{\pi}(s)=\underset{s_{t}, a_{t} \sim\pi}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}\left(r\left(s_{t},a_{t}\right)\right) \mid s_{0}=s\right]\]</span></p><p>2）类似的，可以定义出最大熵版本的Q和V： <span class="math display">\[\begin{aligned} Q_{s o f t}^{\pi}(s, a) =\underset{s_{t}, a_{t} \sim\rho_{\pi}}{\mathbb{E}}\left[\sum_{t=0}^{\infty} \gamma^{t}r\left(s_{t}, a_{t}\right)+\alpha \sum_{t=1}^{\infty} \gamma^{t}H\left(\pi\left(\cdot \mid s_{t}\right)\right) \mid s_{0}=s,a_{0}=a\right] \end{aligned}\]</span></p><p><span class="math display">\[V_{s o f t}^{\pi}(s)=\underset{s_{t}, a_{t} \sim\rho_{\pi}}{\mathbb{E}}\left[\sum_{t=0}^{\infty}\gamma^{t}\left(r\left(s_{t}, a_{t}\right)+\alpha H\left(\pi\left(\cdot\mid s_{t}\right)\right)\right) \mid s_{0}=s\right]\]</span></p><p>从Q的定义出发，同样可以推出贝尔曼方程：</p><p><span class="math display">\[\begin{aligned} Q_{s o f t}^{\pi}(s, a)&amp;=\underset{\substack{s^{\prime} \sim p\left(s^{\prime} \mid s,a\right) \\ a^{\prime} \sim \pi}}{\mathbb{E}}\left[r(s,a)+\gamma\left(Q_{\text {soft }}^{\pi}\left(s^{\prime},a^{\prime}\right)+\alpha H\left(\pi\left(\cdot \mids^{\prime}\right)\right)\right)\right] \\ &amp;=\underset{s^{\prime}\sim p\left(s^{\prime} \mid s, a\right)}{\mathbb{E}}\left[r(s, a)+\gammaV_{s o f t}^{\pi}\left(s^{\prime}\right)\right] \end{aligned}\]</span></p><p>其中</p><p><span class="math display">\[\begin{aligned} V_{s o f t}^{\pi}(s) &amp;=\mathbb{E}_{a \sim\pi}\left[Q_{s o f t}^{\pi}(s, a)\right]+\alpha H(\pi(\cdot \mid s))\\&amp;=\mathbb{E}_{a \sim \pi}\left[Q_{s o f t}^{\pi}(s,a)\right]-\alpha \mathbb{E}_{a \sim \pi}\left[(\log \pi(a \mid s)\right]\\ &amp;=\underset{a \sim \pi}{\mathbb{E}}\left[Q_{s o ft}^{\pi}(s, a)-\alpha \log \pi(a \mid s)\right] \end{aligned}\]</span></p><blockquote><p>上面的最大熵Q、V的定义用在SAC中，而Soft QLearning一开始用的是另一套式子</p></blockquote><h3 id="策略的形式">1.5 策略的形式</h3><p>在1.3、1.4节我们知道了值函数的形式，问题在于，我们最终的目的还是要得到一个策略，那<strong>策略是什么形式呢？</strong></p><p>还记得价值函数<span class="math inline">\(V\)</span>的表示不？</p><p><span class="math display">\[V\left(\mathbf{s}_{t}\right)=\mathbb{E}_{\mathbf{a}_{t} \sim\pi}\left[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\alpha \log\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right]\]</span> 从<spanclass="math inline">\(V\)</span>的表达式中可以反解出<spanclass="math inline">\(\pi\)</span>的形式，移项后可得：</p><p><span class="math display">\[\mathbb{E}_{\mathbf{a}_{t} \sim \pi}\left[Q\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)-\alpha\log \pi\left(\mathbf{a}_{t} \mid\mathbf{s}_{t}\right)-V\left(\mathbf{s}_{t}\right)\right]=0\]</span> <strong>其中一个解为</strong> <span class="math display">\[\begin{aligned} \pi\left(s_{t}|a_{t}\right) &amp;=\exp\left(\frac{1}{\alpha}\left(Q_{s o f t}\left(s_{t}, a_{t}\right)-V_{s of t}\left(s_{t}\right)\right)\right) \\ &amp;=\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{\exp \left(\frac{1}{\alpha} V_{\text {soft}}\left(s_{t}\right)\right)} \end{aligned}\]</span></p><p>于是我们得到了策略的形式。</p><blockquote><p>注意，策略的形式不一定是这个，但是<strong>这样的策略一定满足最大熵的值函数约束，这意味着我们可以直接采用这种形式的策略来进行策略迭代。</strong></p></blockquote><h3 id="状态价值函数v的另一种形式"><strong>1.6状态价值函数V的另一种形式</strong></h3><p>注意到1.5节推导出的策略形式： <span class="math display">\[\begin{aligned} \pi\left(s_{t}|a_{t}\right) =\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{\exp \left(\frac{1}{\alpha} V_{\text {soft}}\left(s_{t}\right)\right)} \end{aligned}\]</span></p><p>由于策略所有动作概率积分为1：</p><p><span class="math display">\[\int\pi\left(s_{t}|a_{t}\right)d \mathbf{a} =\frac{\int\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)d a}{\exp \left(\frac{1}{\alpha} V_{\text {soft}}\left(s_{t}\right)\right)}=1\]</span></p><p>可以直接得到<span class="math inline">\(V\)</span>与<spanclass="math inline">\(Q\)</span>的第二种关系：</p><p><span class="math display">\[V_{\text {soft }}\left(s_{t}\right) \triangleq \alpha \log \int \exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t}, a\right)\right) d a\]</span></p><blockquote><p><strong>注意，这种V和Q的关系的前提在于，策略π取1.5节推导出的特殊形式。</strong><strong>这也是Soft Q Learning中采用的形式</strong></p></blockquote><p>将上式回代得<spanclass="math inline">\(\pi\left(s_{t}|a_{t}\right)\)</span>的另一种形式：<span class="math display">\[\begin{aligned} \pi\left(s_{t}|a_{t}\right)=\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{\exp \left(\frac{1}{\alpha} V_{\text {soft}}\left(s_{t}\right)\right)} =\frac{\exp \left(\frac{1}{\alpha} Q_{\text{soft }}\left(s_{t}, a_{t}\right)\right)}{ \int \exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t}, a\right)\right) d\mathbf{a}} \end{aligned}\]</span></p><h3 id="最优策略"><strong>1.7 最优策略</strong></h3><p>注意到最大熵的策略就是最大化<spanclass="math inline">\(V\)</span>的策略，因此直接对<spanclass="math inline">\(V\)</span>求<spanclass="math inline">\(\pi\)</span>的泛函导数，令其为0，得到的就是最优策略：</p><p><span class="math display">\[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\alpha\log\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)-\alpha=0\\\begin{aligned} \pi\left(s_{t}|a_{t}\right) =\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{e} \end{aligned}\]</span> 又因为</p><p><span class="math display">\[\int\pi\left(s_{t}|a_{t}\right)d \mathbf{a} =\frac{\int\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)d a}{e}=1\\\int\exp \left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right) da=e\]</span></p><p>将分母中的e用上式代替，所以</p><p><span class="math display">\[\begin{aligned} \pi\left(s_{t}|a_{t}\right) =\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{ \int \exp \left(\frac{1}{\alpha} Q_{\text {soft}}\left(s_{t}, a\right)\right) d \mathbf{a}} \end{aligned}\]</span></p><p>而1.6节推出的符合条件的策略形式为：</p><p><span class="math display">\[\begin{aligned} \pi\left(s_{t}|a_{t}\right)=\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{\exp \left(\frac{1}{\alpha} V_{\text {soft}}\left(s_{t}\right)\right)} =\frac{\exp \left(\frac{1}{\alpha} Q_{\text{soft }}\left(s_{t}, a_{t}\right)\right)}{ \int \exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t}, a\right)\right) d\mathbf{a}} \end{aligned}\]</span> 符合最优策略的形式</p><p>因此，<strong>1.5节推出的策略是最优策略。</strong></p><p><strong>又因为在Soft QLearing论文附录中的A.2，最优策略是唯一的。</strong></p><p><strong>因此，1.5节推出的策略是唯一的最优策略。</strong></p><p><strong>至此，我们已经完全分析推导了最大熵框架下的价值函数V、Q，策略π的形式。</strong></p><h2 id="能量模型"><strong>2、能量模型</strong></h2><p>还记得最大熵模型中推导出的<strong>最优策略形式</strong>不？</p><p><span class="math display">\[\begin{aligned} \pi\left(s_{t}|a_{t}\right) =\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{\exp \left(\frac{1}{\alpha} V_{\text {soft}}\left(s_{t}\right)\right)} \end{aligned}\]</span></p><p><strong>这到底是个什么东西？</strong></p><p><strong>Soft QLearning中说它是能量模型，为什么这个东西就是能量模型了？</strong></p><p>在推导完公式之后，还得了解我们推导的到底是个什么东西，这就是这节需要讲的能量模型了。</p><h3 id="为什么要使用能量模型">2.1 为什么要使用能量模型？</h3><p>说到能量模型，就不得不提深度学习三巨头之一的<strong>YannLeCun</strong>了。其凭借着能量模型在生成模型领域大行其道，并提出了“能量模型是通向自主人工智能系统的起点”、“已经做好放弃概率论的准备”等言论。</p><p>所以为什么他这么推崇能量模型呢？</p><p>用他<ahref="https://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/publis/orig/lecun-06.pdf">综述</a>里的话来说，就是：</p><blockquote><p>基于能量的学习为许多概率和非概率的学习方法提供了一个统一的框架，特别是图模型和其他结构化模型的非概率训练。<strong>基于能量的学习可以被看作是预测、分类或决策任务的概率估计的替代方法</strong>。由于<strong>不需要适当的归一化</strong>，基于能量的方法避免了概率模型中与估计归一化常数相关的问题。此外，由于没有标准化条件，在学习机器的设计中允许了更多的灵活性。大多数概率模型都可以看作是特殊类型的基于能量的模型，其中能量函数满足一定的归一化条件，<ahref="https://zhida.zhihu.com/search?q=%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0&amp;zhida_source=entity&amp;is_preview=1">损失函数</a>通过学习优化，具有特定的形式。</p></blockquote><h3 id="能量模型的含义">2.2 能量模型的含义</h3><p>对于基本的能量函数而言，给定一个 X ，模型产生与 X 最兼容的Y</p><p><span class="math display">\[Y^*=\arg\min_{Y\in\mathcal{Y}} E(Y,X)\\\]</span></p><p>其中“最兼容”是指：==小能量值==对应于变量的==高度兼容==，而==大能量值==对应于变量的==高度不兼容==。</p><p>即能量函数要满足下面的性质：</p><p><strong>给定一个<spanclass="math inline">\(X\)</span>，输出的最优<spanclass="math inline">\(Y\)</span>值让能量函数最小</strong></p><blockquote><p>能量函数在不同的技术社区中有不同的名称；它们可以被称为对比函数、价值函数或负对数<ahref="https://zhida.zhihu.com/search?q=%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0&amp;zhida_source=entity&amp;is_preview=1">似然函数</a>。<br />能量函数可以是任意的，只要满足最优的<spanclass="math inline">\(Y\)</span>能让能量最小即可。<br />举个例子，监督学习中的图片为<spanclass="math inline">\(X\)</span>，标签为<spanclass="math inline">\(Y_0（X）\)</span>，能量模型<spanclass="math inline">\(E（X，Y）=（Y-Y_0（X))^2\)</span>，输出的Y越接近X的标签，能量越低。此时监督学习的loss就是能量函数</p></blockquote><h3 id="能量函数的建模">2.3 能量函数的建模</h3><p>对于决策任务，例如操纵机器人，只需要系统给予正确答案就能获得最低的能量，其他答案获得的更大能量无关紧要（比如RL中我们只需要选择能量最低的action即可）。然而，一个系统的输出有时必须与另一个系统的输出相结合，或者提供给另一个系统作为输入(或者提供给人类决策者)。唯一一致的方法是将所有可能输出的能量转换成一个标准化的概率分布。最简单、最常用的方法即转化为Gibbs分布（又叫做<ahref="https://zhida.zhihu.com/search?q=%E7%8E%BB%E5%B0%94%E5%85%B9%E6%9B%BC%E5%88%86%E5%B8%83&amp;zhida_source=entity&amp;is_preview=1">玻尔兹曼分布</a>）：</p><p><span class="math display">\[P(Y \mid X)=\frac{e^{-\beta E(Y, X)}}{\int_{y \in \mathcal{Y}} e^{-\betaE(y, X)}}\]</span></p><blockquote><p>玻尔兹曼分布是一种概率分布，它给出一个系统处于某种状态的机率，该几率是此状态下的能量及温度的函。给出为：<br /><span class="math display">\[p_{i} \propto e^{-\varepsilon_{i} /(k T)}\\p_{i}=\frac{1}{Q} e^{-\varepsilon_{i} /(k T)}=\frac{e^{-\varepsilon_{i}/(k T)}}{\sum_{j=1}^{M} e^{-\varepsilon_{j} /(k T)}}*\]</span></p><p>其中 <em><span class="math inline">\(\varepsilon\)</span></em>表示能量，<strong>k</strong>是<strong><ahref="https://zhida.zhihu.com/search?q=玻尔兹曼常数&amp;zhida_source=entity&amp;is_preview=1">玻尔兹曼常数</a></strong>，<strong>T</strong>是系统的绝对温度，<strong>M</strong>是系统可访问的所有状态的数量。</p></blockquote><p>可以看到，能量越低，被选中的概率越高，即被选中的动作越优。</p><p>这满足能量函数的性质，所以<strong>玻尔兹曼分布是基于能量的分布</strong></p><h3 id="基于能量的强化学习"><strong>2.4 基于能量的强化学习</strong></h3><p><strong>终于可以知道最大熵模型中的最优策略的含义是什么了：</strong></p><p><span class="math display">\[\begin{aligned} \pi\left(s_{t}|a_{t}\right) =\frac{\exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t},a_{t}\right)\right)}{\exp \left(\frac{1}{\alpha} V_{\text {soft}}\left(s_{t}\right)\right)}=\frac{\exp \left(\frac{1}{\alpha} Q_{\text{soft }}\left(s_{t}, a_{t}\right)\right)}{ \int \exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t}, a\right)\right) d\mathbf{a}} \end{aligned}\]</span> 如果取能量函数：</p><p><span class="math display">\[\mathcal{E}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)=-\frac{1}{\alpha}Q_{\operatorname{soft}}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\]</span> 再让策略正比于负能量：</p><p><span class="math display">\[\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \propto \exp\left(-\mathcal{E}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\right)\]</span> 这不就是玻尔兹曼分布吗？其中不存在温度<spanclass="math inline">\(kT\)</span>对系统概率的影响。</p><p>所以<strong>最大熵强化学习中的最优策略形式是玻尔兹曼分布</strong>，其中$$是温度系数。 <span class="math inline">\(\alpha\)</span>越大，各个动作被选中的概率差就越小，越接近随机策略； <spanclass="math inline">\(\alpha\)</span>越小，各个动作被选中的概率差就越大，越接近确定性策略。通过调节 <spanclass="math inline">\(\alpha\)</span> 可以调节策略的随机性探索。</p><blockquote><p>把强化学习的策略建模为玻尔兹曼分布的想法之前就有过很多，并被称为玻尔兹曼探索，这样子做有个好处就是用随机策略可以加强探索，并且满足<spanclass="math inline">\(Q\)</span>越大被选中的概率越大的条件。</p></blockquote><h3 id="能量策略的好处">2.5 能量策略的好处</h3><p>这时候就要祭出<ahref="https://link.zhihu.com/?target=https%3A//bair.berkeley.edu/blog/2017/10/06/soft-q-learning/">这张</a>经典的图了：</p><p><imgsrc="https://picx.zhimg.com/v2-9a97eee9111031a967cf5823b07087ef_b.jpg" /></p><p>能量策略</p><p>传统的基于高斯分布的策略只能捕获单峰的Q值，也就是只能捕获一种行为模式，这种玻尔兹曼分布的能量策略能够捕获多个行为模式，大大增强了表达能力。</p><p>经过前面一大通的推导，现在我们已经理解了基于最大熵和能量模型的强化学习框架，也知道了其中值函数、策略的表达式。</p><p>接下来，根据这个框架，可以导出两个算法。</p><p>第一种，就是类似Q Learning风格的，Soft Q Learning。</p><h3 id="soft-值迭代">3.1 Soft 值迭代</h3><p>再次请出我们的Q和V函数：</p><p><span class="math display">\[\begin{aligned} Q_{s o f t}(s, a) =\underset{s^{\prime} \simp\left(s^{\prime} \mid s, a\right)}{\mathbb{E}}\left[r(s, a)+\gamma V_{so f t}\left(s^{\prime}\right)\right] \end{aligned}\]</span></p><p><span class="math display">\[V_{\text {soft }}\left(s_{t}\right) \triangleq \alpha \log \int \exp\left(\frac{1}{\alpha} Q_{\text {soft }}\left(s_{t}, a\right)\right) d\mathbf{a}\]</span></p><p>那么如何迭代更新得到他们呢？</p><p>只需要把“=”号改成迭代的形式即可：</p><p><span class="math display">\[\begin{aligned} &amp;Q_{\mathrm{soft}}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right) \leftarrow r_{t}+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \simp_{\mathbf{s}}}\left[V_{\mathrm{soft}}\left(\mathbf{s}_{t+1}\right)\right],\forall \mathbf{s}_{t}, \mathbf{a}_{t}\\&amp;V_{\mathrm{soft}}\left(\mathbf{s}_{t}\right) \leftarrow \alpha\log \int_{\mathcal{A}} \exp \left(\frac{1}{\alpha}Q_{\mathrm{soft}}\left(\mathbf{s}_{t}, \mathbf{a}^{\prime}\right)\right)d \mathbf{a}^{\prime}, \forall \mathbf{s}_{t} \end{aligned}\]</span></p><blockquote><p>收敛性证明见<ahref="https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v70/haarnoja17a/haarnoja17a.pdf">SQL附录</a>，思路是利用<ahref="https://zhuanlan.zhihu.com/p/458151225">压缩映射</a>的证明方式。</p></blockquote><p>只要不断地这么更新，值函数就会收敛。</p><h3 id="参数化的soft-值优化">3.2 参数化的Soft 值优化</h3><p>在离散的情况下，只需要进行soft 价值迭代就行了。</p><p>为了进一步增强值函数的表示能力，可以将其用 <spanclass="math inline">\(\theta\)</span> 参数化（例如神经网络参数）变成<spanclass="math inline">\(Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)\)</span>和 $V_{}^{}(_{t})$，使其适应连续的状态空间，并且可以使用梯度下降法来优化。</p><p>V：</p><p>求V的话需要对动作进行积分，这在动作空间特别大或者连续动作空间的时候是不可能的，因此可以考虑使用<ahref="https://zhida.zhihu.com/search?q=%E9%87%8D%E8%A6%81%E6%80%A7%E9%87%87%E6%A0%B7&amp;zhida_source=entity&amp;is_preview=1">重要性采样</a>对其分布进行估计：</p><p><span class="math display">\[V_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t}\right)=\alpha \log\mathbb{E}_{q_{\mathbf{a}^{\prime}}}\left[\frac{\exp\left(\frac{1}{\alpha} Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t},\mathbf{a}^{\prime}\right)\right)}{q_{\mathbf{a}^{\prime}}\left(\mathbf{a}^{\prime}\right)}\right]\]</span></p><p>$q_{^{}} $的话可以是任意分布，初期可以用随机分布，后面可以用当前策略。</p><p>Q：</p><p>求Q的话只需要类似DQN的更新就行了：</p><p><span class="math display">\[J_{Q}(\theta)=\mathbb{E}_{\mathbf{s}_{t} \sim q_{\mathrm{s}_{t}},\mathbf{a}_{t} \simq_{\mathbf{a}_{t}}}\left[\frac{1}{2}\left(\hat{Q}_{\mathrm{soft}}^{\bar{\theta}}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)-Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)\right)^{2}\right]\]</span></p><p>其中</p><p><span class="math display">\[\hat{Q}_{\mathrm{soft}}^{\bar{\theta}}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)=r_{t}+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \simp_{\mathrm{s}}}\left[V_{\mathrm{soft}}^{\bar{\theta}}\left(\mathbf{s}_{t+1}\right)\right]\]</span></p><p>对他们进行随机梯度下降就行了。</p><p><spanclass="math inline">\(q_{\mathrm{s}_{t}}\)</span>是当前策略。</p><p><strong>但是还有一个问题</strong>，策略表示是玻尔兹曼分布： <spanclass="math display">\[\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right) \propto\exp\left(\frac{1}{\alpha} Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)\right)\]</span></p><p>如果是<ahref="https://zhida.zhihu.com/search?q=%E7%A6%BB%E6%95%A3%E7%A9%BA%E9%97%B4&amp;zhida_source=entity&amp;is_preview=1">离散空间</a>还好，<strong>连续空间怎么从这个玻尔兹曼分布进行抽样呢</strong>？</p><p>换言之，<strong>玻尔兹曼分布怎么用策略表示呢</strong>？</p><h3 id="策略的近似采样">3.3 策略的近似采样</h3><p>作者使用了一个可以用来表示策略的网络来代替玻尔兹曼分布的策略进行采样。随后利用KL散度来缩小用策略网络<spanclass="math inline">\(\pi^\phi\)</span>与基于能量的策略之间的差距：</p><p><span class="math display">\[\begin{aligned} J_{\pi}\left(\phi ; \mathbf{s}_{t}\right)=\mathrm{D}_{\mathrm{KL}}\left(\pi^{\phi}\left(\cdot \mid\mathbf{s}_{t}\right) \| \exp\left(\frac{1}{\alpha}\left(Q_{\mathrm{soft}}^{\theta}\left(\mathbf{s}_{t},\cdot\right)-V_{\mathrm{soft}}^{\theta}\right)\right)\right)\end{aligned}\]</span></p><p>作者用了<ahref="https://link.zhihu.com/?target=https%3A//proceedings.neurips.cc/paper/2016/file/b3ba8f1bee1238a2f37603d90b58898d-Paper.pdf">变分梯度下降</a>进行优化。</p><blockquote><p>不过这种采样网络在SAC的时候被弃掉了，估计可能是又复杂又用处不大</p></blockquote><p>这个采样网络的存在可以看做是actor-critic中的actor。</p><h3 id="soft-q-learning">3.4 Soft Q Learning</h3><p>OK，现在价值函数和策略的采样更新方式都有了，就可以写算法了：</p><p><imgsrc="https://pica.zhimg.com/v2-51c51ebd61a203d36115fc0ca51b765e_b.jpg" /></p><p>soft q learning</p><h2 id="soft-actor-critic">4、Soft Actor Critic</h2><p>到了喜闻乐见的SAC环节。</p><h3 id="soft-策略迭代policy-iteration">4.1 Soft 策略迭代（PolicyIteration）</h3><p>首先先介绍一下 <strong>Policy Iteration</strong>： 它是一种在 PolicyEvaluation 和 Policy Improvement中交替迭代更新的强化学习方法。用大白话来讲的话，PolicyEvaluation就是在估计某个状态执行某个动作平均能获得多少回报；而PolicyImprovement 则是将策略调整为执行当前状态具有更大回报的动作。</p><p>SAC<strong>抛弃了对动作进行积分的V函数</strong>： <spanclass="math inline">\(V_{\text {soft }}\left(s_{t}\right) \triangleq\alpha \log \int \exp \left(\frac{1}{\alpha} Q_{\text {soft}}\left(s_{t}, a\right)\right) d \mathbf{a}\)</span></p><p>采用了另一种V函数： <span class="math display">\[V\left(\mathbf{s}_{t}\right)=\mathbb{E}_{\mathbf{a}_{t} \sim\pi}\left[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)-\log\pi\left(\mathbf{a}_{t} \mid \mathbf{s}_{t}\right)\right]\]</span></p><blockquote><p>这样做就消除了V对动作积分难的问题</p></blockquote><p>利用强化学习中的<ahref="https://zhuanlan.zhihu.com/p/541571513">策略迭代</a>方法，每次策略改进得到更好的策略，然后为新策略评估一个价值函数，然后继续改进策略，以此类推。就可以得到一个最优策略。</p><p>1）<strong>策略估计（Policy Evaluation）</strong></p><p>首先需要了解贝尔曼方程： <span class="math display">\[Q(s_t,a_t)=r(s_t,a_t)+\gamma\sum_{s^{\prime}\inS}P(s^{\prime}|s)V(s^{\prime})=r(s_t,a_t)+\gamma\mathbb{E}_{\mathbf{s}_{t+1}\simp}\left[V(\mathbf{s}_{t+1})\right]\]</span> 对其==考虑熵==，定义一个考虑熵的<spanclass="math inline">\(Q(s,a)\)</span>为 <spanclass="math inline">\(\mathcal{T}^π Q ( s_t , a_t)\)</span>。<del>不断对价值函数运用贝尔曼算子，价值函数就能够收敛到策略<spanclass="math inline">\(\pi\)</span>下的价值估计：</del> <spanclass="math display">\[\mathcal{T}^{\pi} Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\triangleq r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \simp}\left[V\left(\mathbf{s}_{t+1}\right)\right]，其中V(\mathbf{s}_{t+1})=\mathbb{E}_{\mathbf{a}_{t+1}\sim\pi}\left[Q(\mathbf{s}_{t+1},\mathbf{a}_{t+1})-\log\pi(\mathbf{a}_{t+1}|\mathbf{s}_{t+1})\right]\]</span></p><p><span class="math display">\[\mathcal{T}^{\pi} Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\triangleq r\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \simp,{\mathbf{a}_{t+1}}\sim\pi}\left[Q(\mathbf{s}_{t+1},\mathbf{a}_{t+1})-\log\pi(\mathbf{a}_{t+1}|\mathbf{s}_{t+1})\right]\]</span></p><blockquote><p>证明：把奖励<spanclass="math inline">\(r\)</span>和熵写到一起，因此最大熵目标变成了包含熵的奖励：<span class="math display">\[r_{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \triangleqr\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)+\mathbb{E}_{\mathbf{s}_{t+1} \simp}\left[\mathcal{H}\left(\pi\left(\cdot \mid\mathbf{s}_{t+1}\right)\right)\right]\]</span> 而这种修改了的奖励形式恰好符合策略估计的<ahref="https://zhuanlan.zhihu.com/p/541571513">收敛证明</a>：<br /><span class="math display">\[Q\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right) \leftarrowr_{\pi}\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)+\gamma\mathbb{E}_{\mathbf{s}_{t+1} \sim p, \mathbf{a}_{t+1} \sim\pi}\left[Q\left(\mathbf{s}_{t+1}, \mathbf{a}_{t+1}\right)\right]\]</span> 所以价值函数收敛</p></blockquote><p>2）<strong>策略改进（Policy Improvement）</strong></p><p>在1、2节我们知道，<strong>最优策略是采用玻尔兹曼分布能量形式的策略</strong></p><p>SAC利用这一点来更新策略<strong>：</strong></p><p><span class="math display">\[\pi_{\text {new }}=\arg \min _{\pi^{\prime} \in \Pi}\mathrm{D}_{\mathrm{KL}}\left(\pi^{\prime}\left(\cdot \mid\mathbf{s}_{t}\right) \| \frac{\exp\left(Q^{\pi_{\mathrm{old}}}\left(\mathbf{s}_{t},\cdot\right)\right)}{Z^{\pi_{\text {old}}}\left(\mathbf{s}_{t}\right)}\right)\]</span></p><p>但是并没有直接采用玻尔兹曼分布，而是通过高斯分布来近似玻尔兹曼分布，因为高斯分布更好处理一些。</p><blockquote><p>但是这样直接丢失了玻尔兹曼分布捕获多峰函数的能力</p></blockquote><p><spanclass="math inline">\(Z\)</span>是归一化函数。在上式中，我们先将<spanclass="math inline">\(Q^{\pi_{old}}_{soft}\)</span>指数化为<spanclass="math inline">\(\exp(\alpha^{-1}{Q^{\pi_{old}}_{soft}})\)</span>再将其归一化为分布<spanclass="math inline">\(\frac{\exp\left(Q^{\pi_{\mathrm{old}}}\left(\mathbf{s}_{t},\cdot\right)\right)}{Z^{\pi_{\text {old}}}\left(\mathbf{s}_{t}\right)}\)</span>，其中<spanclass="math inline">\(Z^{\pi_{old}}(s_t)=\int\exp(\alpha^{-1}{Q^{\pi_{old}}_{soft}})da_t\)</span>为归一化函数。我们希望找到一个策略<spanclass="math inline">\(\pi \prime\)</span>在状态<spanclass="math inline">\(s_t\)</span>下的动作分布与<spanclass="math inline">\(\frac{\exp\left(Q^{\pi_{\mathrm{old}}}\left(\mathbf{s}_{t},\cdot\right)\right)}{Z^{\pi_{\text {old}}}\left(\mathbf{s}_{t}\right)}\)</span>分布的KL-divergence最小，即希望<span class="math inline">\(\pi\prime(\cdot|s_t)\)</span>与分布<span class="math inline">\(\frac{\exp\left(Q^{\pi_{\mathrm{old}}}\left(\mathbf{s}_{t},\cdot\right)\right)}{Z^{\pi_{\text {old}}}\left(\mathbf{s}_{t}\right)}\)</span>越相似越好。采用普通的策略优化只会在最高峰满足相似性，而soft策略优化会在整个分布上尽量与价值函数保持相似，不仅保证了赋予动作价值更高的动作更大的概率，还保证了新策略具有较大的熵。过程如下图所示：</p><p><img src="https://pic1.zhimg.com/80/v2-d5fa72ff00bb69449928fba1e6911a8a_1440w.webp" style="zoom: 33%;" /></p><h3 id="参数化的sac算法">4.2 参数化的SAC算法</h3><p>类似Soft Q Learning，SAC也将价值函数 $V_{}(_{t}) $、 <spanclass="math inline">\(Q_{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)\)</span> 和策略 <spanclass="math inline">\(\pi_{\phi}\left(\mathbf{a}_{t} \mid\mathbf{s}_{t}\right)\)</span> 参数化来表达连续域与方便优化。</p><p>1）<strong>价值函数的优化</strong></p><p>价值函数的优化都类似DQN算法，通过极小化Soft Bellmanresidual实现，下式中<spanclass="math inline">\(D\)</span>为回放缓冲区（Replay Buffer）：</p><p>优化<span class="math inline">\(V\)</span>的目标函数：</p><p><span class="math display">\[J_{V}(\psi)=\mathbb{E}_{\mathbf{s}_{t} \sim\mathcal{D}}\left[\frac{1}{2}\left(V_{\psi}\left(\mathbf{s}_{t}\right)-\mathbb{E}_{\mathbf{a}_{t}\sim \pi_{\phi}}\left[Q_{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)-\log \pi_{\phi}\left(\mathbf{a}_{t} \mid\mathbf{s}_{t}\right)\right]\right)^{2}\right]\]</span></p><p>随机梯度：</p><p><span class="math display">\[\hat{\nabla}_{\psi} J_{V}(\psi)=\nabla_{\psi}V_{\psi}\left(\mathbf{s}_{t}\right)\left(V_{\psi}\left(\mathbf{s}_{t}\right)-Q_{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)+\log \pi_{\phi}\left(\mathbf{a}_{t} \mid\mathbf{s}_{t}\right)\right)\]</span></p><p>优化<span class="math inline">\(Q\)</span>的目标函数：</p><p><span class="math display">\[J_{Q}(\theta)=\mathbb{E}_{\left(\mathbf{s}_{t}, \mathbf{a}_{t}\right)\sim \mathcal{D}}\left[\frac{1}{2}\left(Q_{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)-\hat{Q}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)\right)^{2}\right]\]</span></p><p>其中：</p><p><span class="math display">\[\hat{Q}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)=r\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)+\gamma \mathbb{E}_{\mathbf{s}_{t+1} \simp}\left[V_{\bar{\psi}}\left(\mathbf{s}_{t+1}\right)\right]\]</span></p><p>随机梯度：</p><p><span class="math display">\[\hat{\nabla}_{\theta} J_{Q}(\theta)=\nabla_{\theta}Q_{\theta}\left(\mathbf{a}_{t},\mathbf{s}_{t}\right)\left(Q_{\theta}\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)-r\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)-\gammaV_{\bar{\psi}}\left(\mathbf{s}_{t+1}\right)\right)\]</span></p><p>同样用到了target网络<spanclass="math inline">\(V_{\bar{\psi}}\)</span></p><p>2）<strong>策略的优化</strong></p><p><strong>同样用一个高斯分布通过KL散度近似玻尔兹曼分布：</strong> <spanclass="math display">\[J_{\pi}(\phi)=\mathbb{E}_{\mathbf{s}_{t} \sim\mathcal{D}}\left[\mathrm{D}_{\mathrm{KL}}\left(\pi_{\phi}\left(\cdot\mid \mathbf{s}_{t}\right) \| \frac{\exp\left(Q_{\theta}\left(\mathbf{s}_{t},\cdot\right)\right)}{Z_{\theta}\left(\mathbf{s}_{t}\right)}\right)\right]\]</span></p><p>对此有两种好用的优化方法</p><ul><li><strong>策略梯度方法</strong>：SAC的策略梯度参考<ahref="https://zhuanlan.zhihu.com/p/503393367">这篇</a>，是可以推导出来的。</li><li><strong>重参数化技巧，也是SAC实际运用的方法：</strong></li></ul><p><span class="math display">\[\begin{aligned}D_{KL}(p||q)&amp;=H(p,q)-H(p)\\ &amp;=- \sum_x p(x)\log q(x)- (-\sum_xp(x)\log p(x))\\ &amp;=- \sum_x p(x)(\log q(x)-\log p(x))\\ &amp;=- \sum_x p(x)\log \frac{q(x)}{p(x)}\end{aligned}\]</span></p>把策略的目标函数化简： $$<span class="math display">\[\begin{aligned}&amp;\underset{\phi}{\operatorname{minimize}} D_{KL}\left(\pi_\phi\left(\cdot \mid s_t\right) \|  \frac{\exp(\alpha^{-1}Q_\theta\left(s_t, a_t\right))}{\exp(\log\left(Z_\theta\left(s_t\right)\right))}\right)\\&amp;=\underset{\phi}{\operatorname{minimize}} D_{KL}\left(\pi_\phi\left(\cdot \mid s_t\right) \| \exp \left[\alpha^{-1}Q_\theta\left(s_t, a_t\right)-\log\left(Z_\theta\left(s_t\right)\right)\right]\right)\\ &amp;=\underset{\phi}{\operatorname{minimize}} \sum_{a_t}\left[\log\pi_\phi\left(a_t \mid s_t\right)-\alpha ^{-1}Q_\theta\left(s_t,a_t\right)+\log \left(Z_\theta\left(s_t\right)\right]\right.\\ &amp;=\underset{\phi}{\operatorname{minimize}} \underset{a_t \sim\pi_\phi\left(\cdot| s_t\right)}{\mathbb{E}}\left[\log \pi_\phi\left(a_t\mid s_t\right)-\alpha ^{-1}Q_\theta\left(s_t, a_t\right)+\log\left(Z_\theta\left(s_t\right)\right]\right.\\ &amp;=\underset{\phi}{\operatorname{minimize}} \underset{a_t \sim\pi_\phi\left(\cdot| s_t\right)}{\mathbb{E}}\left[\alpha \log\pi_\phi\left(a_t \mid s_t\right)-Q_\theta\left(s_t, a_t\right)\right]\\ \end{aligned}\]</span><p>$$</p><p><span class="math display">\[J_{\pi}(\phi)=E_{s_{t} \sim D, a_{t} \sim \pi_{\phi}}\left[\log_{\pi_{\phi}}\left(a_{t} \mid s_{t}\right)-\frac{1}{\alpha}Q_{\theta}\left(s_{t}, a_{t}\right)\right]\]</span></p><p><strong>如果直接对其求策略梯度的话，是不能把直接对期望里面的项求梯度的，因为期望包含了策略，因此必须对期望进行解耦，使其与策略无关</strong>。</p><p>直接把动作建模为取决于噪声分布和状态分布的网络：</p><p><span class="math display">\[\mathbf{a}_{t}=f_{\phi}\left(\epsilon_{t} ; \mathbf{s}_{t}\right)\]</span></p><p>其中$ _{t} $是取自诸如高斯分布的噪声。</p><p>因此目标函数就变为：</p><p><span class="math display">\[J_{\pi}(\phi)=\mathbb{E}_{\mathbf{s}_{t} \sim \mathcal{D}, \epsilon_{t}\sim \mathcal{N}}\left[\log \pi_{\phi}\left(f_{\phi}\left(\epsilon_{t} ;\mathbf{s}_{t}\right) \mid\mathbf{s}_{t}\right)-Q_{\theta}\left(\mathbf{s}_{t},f_{\phi}\left(\epsilon_{t} ; \mathbf{s}_{t}\right)\right)\right]\]</span></p><p>可以看到期望里的噪声分布和策略参数解耦了</p><p>因此就可以直接对其求梯度了，不用管期望里的分布了：</p><p><span class="math display">\[\begin{aligned} \hat{\nabla}_{\phi} J_{\pi}(\phi)=\nabla_{\phi} \log\pi_{\phi}\left(\mathbf{a}_{t} \mid\mathbf{s}_{t}\right)d+\left(\nabla_{\mathbf{a}_{t}} \log\pi_{\phi}\left(\mathbf{a}_{t} \mid\mathbf{s}_{t}\right)-\nabla_{\mathbf{a}_{t}} Q\left(\mathbf{s}_{t},\mathbf{a}_{t}\right)\right) \nabla_{\phi} f_{\phi}\left(\epsilon_{t} ;\mathbf{s}_{t}\right) \end{aligned}\]</span></p><blockquote><p>注意，<strong>不用重参数化技巧也可以</strong>。如果直接对期望求梯度也是可以做的，那样就利用了策略梯度方法的思想。<br />重参数化技巧的额外好处是方差小，SAC用重参数化技巧其实是这个原因。</p></blockquote><h3 id="sac算法">4.3 SAC算法</h3><p>SAC又从别的算法里抄了几个trick，比如TD3的双Q网络取最小、还有DQN的trick，不过这些都不重要。</p><p>经过上面的分析，价值函数和策略的更新都了解了，可以写出算法了：</p><p><imgsrc="https://pic2.zhimg.com/v2-5c60331b84a610aef81ad3101ff2b2bb_b.jpg" /></p><p>SAC算法</p><h2 id="sac2自动调整温度系数">5、SAC2：自动调整温度系数</h2><p>SAC算法对于温度参数非常敏感，而调整热度参数是又比较困难——我们怎么确定在每一时刻用什么温度参数呢？只需要保证每一步的熵都不低于一个最小值就行了，即求解如下<ahref="https://zhida.zhihu.com/search?q=%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98&amp;zhida_source=entity&amp;is_preview=1">约束优化问题</a>：</p><p><span class="math display">\[\max _{\pi_{0}, \ldots, \pi_{T}} \mathbb{E}\left[\sum_{t=0}^{T}r\left(s_{t}, a_{t}\right)\right] \text { s.t. } \forall t,\mathcal{H}\left(\pi_{t}\right) \geq \mathcal{H}_{0}\]</span></p><p>因为在时刻t的策略不会影响到之前时刻的策略，我们可以用类似动态规划的思想从后往前逐步最大化收益：</p><p><span class="math display">\[\max _{\pi 0}\left(\mathbb{E}\left[r\left(s_{0},a_{0}\right)\right]+\max _{\pi_{1}}\left(\mathbb{E}[\ldots]+\max_{\pi_{T}} \mathbb{E}\left[r\left(s_{T},a_{T}\right)\right]\right)\right)\]</span></p><p>我们从最后一个时刻T开始最大化：</p><p><span class="math display">\[\mathbb{E}_{\left(s_{T}, a_{T}\right) \sim\rho_{\pi}}\left[r\left(s_{T}, a_{T}\right)\right] \text { s.t. }\mathcal{H}\left(\pi_{T}\right)-\mathcal{H}_{0} \geq 0\]</span></p><p>要求解这个问题，需要用到拉格朗日对偶的方法。</p><blockquote><p>拉格朗日对偶这个写的非常好：<ahref="https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/davidknowles/lagrangian_duality.pdf">笨蛋都能看懂的拉格朗日对偶</a><br />知道了拉格朗日对偶的做法，就可以继续下面的推导啦</p></blockquote><p>首先，我们定义以下函数：</p><p><span class="math display">\[\begin{aligned} h\left(\pi_{T}\right)&amp;=\mathcal{H}\left(\pi_{T}\right)-\mathcal{H}_{0}=\mathbb{E}_{\left(s_{T},a_{T}\right) \sim \rho_{\pi}}\left[-\log \pi_{T}\left(a_{T} \mids_{T}\right)\right]-\mathcal{H}_{0} \\ f\left(\pi_{T}\right) &amp;=\begin{cases}\mathbb{E}_{\left(s_{T}, a_{T}\right) \sim\rho_{\pi}}\left[r\left(s_{T}, a_{T}\right)\right], &amp; \text { if }h\left(\pi_{T}\right) \geq 0 \\ -\infty, &amp; \text { otherwise}\end{cases} \end{aligned}\]</span></p><p>于是优化问题就变成了：</p><p><span class="math display">\[f\left(\pi_{T}\right) \text { s.t. } h\left(\pi_{T}\right) \geq 0\]</span></p><p>为了解决这个不等式约束的最大化优化问题，我们可以构建一个带有<ahref="https://zhida.zhihu.com/search?q=%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90&amp;zhida_source=entity&amp;is_preview=1">拉格朗日乘子</a>的表达式：</p><p><span class="math display">\[L\left(\pi_{T}, \alpha_{T}\right)=f\left(\pi_{T}\right)+\alpha_{T}h\left(\pi_{T}\right)\]</span></p><p>实际上，目标函数就等价于最小化L的<spanclass="math inline">\(\alpha\)</span>的式子：</p><p><span class="math display">\[f\left(\pi_{T}\right)=\min _{\alpha_{T} \geq 0} L\left(\pi_{T},\alpha_{T}\right)\]</span></p><p>这样写的原因如下：</p><ul><li>如果约束被满足，那么 <span class="math inline">\(\alpha\)</span>最好将其设置为0才能让L最小（注意 $ $ ）。于是。</li><li>如果约束被违背了，即，我们可以通过令<spanclass="math inline">\(\alpha_{T} \rightarrow \infty\)</span>来使得$L(<em>{T}, </em>{T}) -<spanclass="math inline">\(。于是\)</span>L(<em>{T}, )=-=f(</em>{T})$。</li></ul><p><strong>上面这个min仅仅代表的是约束条件的满足</strong></p><p><strong>为了最大化目标函数还需要一个max的操作：</strong> <spanclass="math display">\[\max _{\pi_{T}} f\left(\pi_{T}\right)=\max _{\pi_{T}}\min _{\alpha_{T}\geq 0} L\left(\pi_{T}, \alpha_{T}\right)\]</span> 现在把它变成更好处理的对偶问题：</p><p><span class="math display">\[\begin{aligned} \max _{\pi_{T}} \mathbb{E}\left[r\left(s_{T},a_{T}\right)\right] &amp;=\max _{\pi_{T}} f\left(\pi_{T}\right) \\&amp;=\min _{\alpha_{T} \geq 0} \max _{\pi_{T}} L\left(\pi_{T},\alpha_{T}\right) \\ &amp;=\min _{\alpha_{T} \geq 0} \max _{\pi_{T}}f\left(\pi_{T}\right)+\alpha_{T} h\left(\pi_{T}\right) \\ &amp;=\min_{\alpha_{T} \geq 0} \max _{\pi_{T}} \mathbb{E}_{\left(s_{T},a_{T}\right) \sim \rho_{\pi}}\left[r\left(s_{T},a_{T}\right)\right]+\alpha_{T}\left(\mathbb{E}_{\left(s_{T},a_{T}\right) \sim \rho_{\pi}}\left[-\log \pi_{T}\left(a_{T} \mids_{T}\right)\right]-\mathcal{H}_{0}\right) \\ &amp;=\min _{\alpha_{T}\geq 0} \max _{\pi_{T}} \mathbb{E}_{\left(s_{T}, a_{T}\right) \sim\rho_{\pi}}\left[r\left(s_{T}, a_{T}\right)-\alpha_{T} \log\pi_{T}\left(a_{T} \mid s_{T}\right)\right]-\alpha_{T} \mathcal{H}_{0}\\ &amp;=\min _{\alpha_{T} \geq 0} \max _{\pi_{T}}\mathbb{E}_{\left(s_{T}, a_{T}\right) \sim\rho_{\pi}}\left[r\left(s_{T}, a_{T}\right)+\alpha_{T}\mathcal{H}\left(\pi_{T}\right)-\alpha_{T} \mathcal{H}_{0}\right]\end{aligned}\]</span></p><blockquote><p>由于目标函数是线性的，熵约束是<ahref="https://zhida.zhihu.com/search?q=%E5%87%B8%E5%87%BD%E6%95%B0&amp;zhida_source=entity&amp;is_preview=1">凸函数</a>，所以强对偶性成立。<br />这意味着对偶问题的最优解等于原始问题的最优解。</p></blockquote><p>所以最优策略和最优温度系数都可以表示出来了：</p><p><span class="math display">\[\begin{aligned} &amp;\pi_{T}^{\star}=\arg \max _{\pi_{T}}\mathbb{E}_{\left(s_{T}, a_{T}\right) \sim \rho_{\pi}}\left[\left(s_{T},a_{T}\right)+\alpha_{T} \mathcal{H}\left(\pi_{T}\right)-\alpha_{T}\mathcal{H}_{0}\right] \\ &amp;\alpha_{T}^{\star}=\arg \min _{\alpha_{T}\geq 0} \mathbb{E}_{\left(s_{T}, a_{T}\right) \sim\rho_{\pi^{\star}}}\left[\alpha_{T}\mathcal{H}\left(\pi_{T}^{\star}\right)-\alpha_{T}\mathcal{H}_{0}\right] \end{aligned}\]</span></p><p>所以目标函数的最大化完全可以用最优策略和温度系数表示：</p><p><span class="math display">\[\max _{\pi_{T}} \mathbb{E}\left[r\left(s_{T},a_{T}\right)\right]=\mathbb{E}_{\left(s_{T}, a_{T}\right) \sim\rho_{\pi^{\star}}}\left[r\left(s_{T}, a_{T}\right)+\alpha_{T}^{\star}\mathcal{H}\left(\pi_{T}^{\star}\right)-\alpha_{T}^{\star}\mathcal{H}_{0}\right]\]</span></p><p>把这个思想用到Q函数中，那么Q函数也可以这么表示：</p><p><span class="math display">\[\begin{aligned} Q_{T-1}\left(s_{T-1}, a_{T-1}\right)&amp;=r\left(s_{T-1}, a_{T-1}\right)+\mathbb{E}\left[Q\left(s_{T},a_{T}\right)-\alpha_{T} \log \pi\left(a_{T} \mid s_{T}\right)\right] \\&amp;=r\left(s_{T-1}, a_{T-1}\right)+\mathbb{E}\left[r\left(s_{T},a_{T}\right)\right]+\alpha_{T} \mathcal{H}\left(\pi_{T}\right) \\Q_{T-1}^{\star}\left(s_{T-1}, a_{T-1}\right) &amp;=r\left(s_{T-1},a_{T-1}\right)+\max _{\pi_{T}} \mathbb{E}\left[r\left(s_{T},a_{T}\right)\right]+\alpha_{T}^{\star}\mathcal{H}\left(\pi_{T}^{\star}\right) \end{aligned}\]</span>因此,回溯到T-1步，把最优Q的式子带进去，形成了另一个约束优化问题</p><p><span class="math display">\[\begin{aligned} &amp;\max_{\pi_{T-1}}\left(\mathbb{E}\left[r\left(s_{T-1},a_{T-1}\right)\right]+\max _{\pi_{T}} \mathbb{E}\left[r\left(s_{T},a_{T}\right]\right)\right. \\ &amp;=\max_{\pi_{T-1}}\left(Q_{T-1}^{\star}\left(s_{T-1},a_{T-1}\right)-\alpha_{T}^{\star}\mathcal{H}\left(\pi_{T}^{\star}\right)\right),s.t.\mathcal{H}\left(\pi_{T-1}\right)-\mathcal{H}_{0}\geq 0 \end{aligned}\]</span></p><p>同样使用拉格朗日对偶：</p><p><span class="math display">\[\begin{aligned} &amp;\max_{\pi_{T-1}}\left(\mathbb{E}\left[r\left(s_{T-1},a_{T-1}\right)\right]+\max _{\pi_{T}} \mathbb{E}\left[r\left(s_{T},a_{T}\right]\right)\right. \\ &amp;=\max_{\pi_{T-1}}\left(Q_{T-1}^{\star}\left(s_{T-1},a_{T-1}\right)-\alpha_{T}^{\star}\mathcal{H}\left(\pi_{T}^{\star}\right)\right) \\ &amp;=\min_{\alpha_{T-1} \geq 0} \max_{\pi_{T-1}}\left(Q_{T-1}^{\star}\left(s_{T-1},a_{T-1}\right)-\alpha_{T}^{\star}\mathcal{H}\left(\pi_{T}^{\star}\right)+\alpha_{T-1}\left(\mathcal{H}\left(\pi_{T-1}\right)-\mathcal{H}_{0}\right)\right)\\ &amp;=\min _{\alpha_{T-1} \geq 0} \max_{\pi_{T-1}}\left(Q_{T-1}^{\star}\left(s_{T-1},a_{T-1}\right)+\alpha_{T-1}\mathcal{H}\left(\pi_{T-1}\right)-\alpha_{T-1}\mathcal{H}_{0}\right)-\alpha_{T}^{\star}\mathcal{H}\left(\pi_{T}^{\star}\right) \end{aligned}\]</span> 类似的，也可以得到T-1步的最优策略和最优温度系数：</p><p><span class="math display">\[\begin{aligned} \pi_{T-1}^{\star} &amp;=\arg \max _{\pi_{T-1}}\mathbb{E}_{\left(s_{T-1} a_{T-1}\right) \sim\rho_{\pi}}\left[Q_{T-1}^{\star}\left(s_{T-1},a_{T-1}\right)+\alpha_{T-1}\mathcal{H}\left(\pi_{T-1}\right)-\alpha_{T-1} \mathcal{H}_{0}\right] \\\alpha_{T-1}^{\star} &amp;=\arg \min _{\alpha_{T-\mathbb{Z}}}\mathbb{E}_{\left(s_{T-1} a_{T-1}\right) \sim\rho_{\pi^{\star}}}\left[\alpha_{T-1}\mathcal{H}\left(\pi_{T-1}^{\star}\right)-\alpha_{T-1}\mathcal{H}_{0}\right] \end{aligned}\]</span></p><p>于是，在每一步我们都能够最小化以下目标函数以求解温度系数：</p><p><span class="math display">\[J(\alpha)=\mathbb{E}_{a_{t} \sim \pi_{t}}\left[-\alpha \log\pi_{t}\left(a_{t} \mid \pi_{t}\right)-\alpha \mathcal{H}_{0}\right]\]</span></p><p>并得到最优温度系数：</p><p><span class="math display">\[\alpha_{t}^{*}=\arg \min _{\alpha_{t}} \mathbb{E}_{\mathbf{a}_{t} \sim\pi_{t}^{*}}\left[-\alpha_{t} \log \pi_{t}^{*}\left(\mathbf{a}_{t} \mid\mathbf{s}_{t} ; \alpha_{t}\right)-\alpha_{t}\overline{\mathcal{H}}\right]\]</span></p><p>综上，把熵的自动求解放入SAC中，就形成了SAC2算法：</p><p><imgsrc="https://pic1.zhimg.com/v2-562d7f3a5bced03cec813668ab178eac_b.jpg" /></p><p>SAC2</p><h2 id="sac-离散">6、SAC-离散</h2><p>SAC当然是一个很好的想法，在很多连续动作空间的任务上都达到了SOTA，不过并不是很适合离散动作空间。</p><p>为了适应离散动作空间，必须要做出以下五个改变：</p><ul><li><strong>Q函数输出所有的状态动作</strong>。这在连续状态空间是不可能的，因为动作是连续的。在离散动作空间中这种情况就成为了提高效率的一种可能方法。</li><li><strong>策略函数不输出均值和方差，直接输出动作分布</strong>。这是离散动作空间的必然要求。</li><li><strong>对求V动作的期望改成积分</strong>。之前求V的时候需要对动作求期望：$V(s_{t}):=E_{a_{t}}<spanclass="math inline">\(，不过在离散动作空间，所有动作的积分是可以求得的，因此变期望为积分以减少方差：\)</span>V(s_{t}):=(s_{t})^{T}$</li><li><strong>同样的，对温度系数的动作期望改成积分</strong>。<spanclass="math inline">\(J(\alpha)=\pi_{t}\left(s_{t}\right)^{T}\left[-\alpha\left(\log\left(\pi_{t}\left(s_{t}\right)\right)+\bar{H}\right)\right]\)</span></li><li><strong>去掉重参数化</strong>。之前使用重参数化的一个原因是期望里带动作分布，因此梯度不能从期望外直接提到期望内。但是现在动作已经变成积分/求和形式了，每个动作都能直接计算出来，因此可以直接对以下目标函数求梯度：<spanclass="math inline">\(J_{\pi}(\phi)=E_{s_{t} \simD}\left[\pi_{t}\left(s_{t}\right)^{T}\left[\alpha \log\left(\pi_{\phi}\left(s_{t}\right)\right)-Q_{\theta}\left(s_{t}\right)\right]\right]\)</span></li></ul><p>这五点构成了离散SAC算法：</p><p><imgsrc="https://pic3.zhimg.com/v2-1c5c687228798d928f0c5ce178fef97e_b.jpg" /></p><p>离散SAC</p><h1 id="参考">参考</h1><p><a href="https://zhuanlan.zhihu.com/p/557418338">从Soft QLearning到SAC - 知乎 (zhihu.com)</a></p><script src="https://utteranc.es/client.js"        repo="Antonalia/antonalia.github.io"        issue-term="pathname"        label="comment"        theme="github-light"        crossorigin="anonymous"        async></script>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>SAC算法</tag>
      
      <tag>Actor-Critic算法</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>归一化、标准化、正则化</title>
    <link href="/03normalization/"/>
    <url>/03normalization/</url>
    
    <content type="html"><![CDATA[<h2 id="归一化normalization">归一化（Normalization）</h2><p> ①把数据变为（0，1）或（-1，1）之间的小数。主要是为了方便数据处理，因为将数据映射到0～1范围之内，可以使处理过程更加便捷、快速。</p><p> ②把有量纲表达式变换为无量纲表达式，成为纯量。经过归一化处理的数据，处于同一数量级，可以消除指标之间的量纲和量纲单位的影响，提高不同数据指标之间的可比性。</p><p><strong>主要算法</strong>：</p><p> ①线性转换，即min-max归一化（常用方法）<br /><span class="math display">\[y=\frac{x-min}{max-min}\]</span></p><p> ②对数函数转换 <span class="math display">\[y=\lg x\]</span></p><h2 id="标准化standardization">标准化（Standardization）</h2><p>  数据的标准化是将数据按比例缩放，使之落入一个小的特定区间。</p><p><strong>主要算法</strong>：</p><p> ①z-score标准化，即零-均值标准化（常用方法）,其中<spanclass="math inline">\(\mu\)</span>是样本数据的<strong>均值（mean）</strong>，<spanclass="math inline">\(\sigma\)</span>是样本数据的<strong>标准差（std）</strong><span class="math display">\[y=\frac{x-\mu}{\sigma}\]</span></p><figure><img src="/images/norm.webp" alt="散点序列的标准化过程" /><figcaption aria-hidden="true">散点序列的标准化过程</figcaption></figure><p>上图则是一个散点序列的标准化过程：原图-&gt;减去均值-&gt;除以标准差。</p><p>这是一种统计的处理，基于正态分布的假设，将数据变换为均值为0、标准差为1的标准正态分布。但即使数据不服从正态分布，也可以用此法。特别适用于数据的最大值和最小值未知，或存在孤立点。</p><p> ②小数定标标准化</p><p>  小数定标标准化通过移动x的小数位置进行标准化，将数据映射到[-1,1]区间上，移动的小数位数取决于数据绝对值的最大值。例如一组数据为[99，10，210，-90，-999]，其中绝对值最大数为-999，那么所有数据小数点移动三位即可得到标准化后的数据[0.099，0.01，0.21，-0.09，-0.999]。<span class="math display">\[y=\frac{x}{10^j},j 确保max(|y|)&lt;1\]</span></p><p> ③ 对数Logistic模式 <span class="math display">\[y=\frac{1}{1+e^{-x}}\]</span></p><h2 id="正则化regularization">正则化（Regularization）</h2><p>  用一组与原不适定问题相“邻近”的适定问题的解，去逼近原问题的解，这种方法称为正则化方法。如何建立有效的正则化方法是反问题领域中不适定问题研究的重要内容。通常的正则化方法有基于变分原理的Tikhonov正则化、各种迭代方法以及其它的一些改进方法。</p><p>  总的来说，归一化是为了<u>消除不同数据之间的量纲，方便数据比较和共同处理</u>，比如在神经网络中，归一化可以加快训练网络的收敛性；标准化是为了<u>方便数据的下一步处理，而进行的数据缩放等变换</u>，并不是为了方便与其他数据一同处理或比较，比如数据经过零-均值标准化后，更利于使用标准正态分布的性质，进行处理；正则化而是利用先验知识，在处理过程中引入正则化因子(regulator)，增加引导约束的作用，比如在逻辑回归中使用正则化，可有效降低过拟合的现象。</p><h1 id="参考">参考</h1><p><a href="https://www.jianshu.com/p/95a8f035c86c">归一化（Normalization）、标准化 （Standardization）和中心化/零均值化（Zero-centered） - 简书 (jianshu.com)</a></p><script src="https://utteranc.es/client.js"        repo="Antonalia/antonalia.github.io"        issue-term="pathname"        label="comment"        theme="github-light"        crossorigin="anonymous"        async></script>]]></content>
    
    
    <categories>
      
      <category>学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>数据处理</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>个人博客人 Hexo和Fluid的安装教程</title>
    <link href="/02hexo-fluid-install/"/>
    <url>/02hexo-fluid-install/</url>
    
    <content type="html"><![CDATA[<p>使用命令 ping github.com 检查能否连接到GitHub</p><p>nodejs安装地址 https://nodejs.org/en/</p><p>GitHub Pages +Hexo搭建个人博客网站，史上最全教程，不要配置leancloud，无法使用https://blog.csdn.net/yaorongke/article/details/119089190?spm=1001.2014.3001.5506</p><p>使用 Utterances 为静态博客添加评论，每篇文章在最后添加链接https://roife.github.io/posts/use-utterances-for-blog-comment/#:~:text=%E6%89%93%E5%BC%80%20utterances%20-%20GitHub%20App%20%E7%82%B9%E5%87%BB%20Install%20%E8%BF%9B%E5%85%A5%E5%AE%89%E8%A3%85%E9%A1%B5%E9%9D%A2%E3%80%82,Only%20select%20repositories%20%EF%BC%8C%E5%B9%B6%E5%9C%A8%E4%B8%8B%E6%8B%89%E6%A1%86%E4%B8%AD%E9%80%89%E6%8B%A9%E8%87%AA%E5%B7%B1%E7%9A%84%E5%8D%9A%E5%AE%A2%E4%BB%93%E5%BA%93%EF%BC%88%E6%AF%94%E5%A6%82%E6%88%91%E5%B0%B1%E6%98%AF%20roife%2Froife.github.io%20%EF%BC%8C%E4%B9%9F%E5%8F%AF%E4%BB%A5%E5%AE%89%E8%A3%85%E5%88%B0%E5%85%B6%E4%BB%96%E4%BB%93%E5%BA%93%EF%BC%89%EF%BC%8C%E7%84%B6%E5%90%8E%E7%82%B9%E5%87%BB%20Install%20%E3%80%82基于fluid主题的Hexo博客中添加评论https://lizhening.github.io/posts/852186e5/ 让你的 Hexo 博客更美观的 N种配置（基于 Fluid 主题扩展） https://www.jianshu.com/p/ba692a97a602</p><p>hexo使用指南，有修改文件html路径的方法https://fuguigui.github.io/hexo2/#</p><p>hexo fluid用户手册https://fluid-dev.github.io/hexo-fluid-docs/guide/#%E5%88%9B%E5%BB%BA%E9%A1%B5%E9%9D%A2</p><p>fluid主题配置https://youlan-lan.github.io/2021/06/07/Hexo%20%E4%B8%BB%E9%A2%98%E4%B9%8B%20Fluid/</p><p>直接修改node_global和node_cache文件夹的权限https://blog.csdn.net/HANZY72/article/details/122505375</p><p>hexo 中如何控制首页/归档页/tag页中显示的文章数https://tanjuntao.github.io/2020/02/28/hexo-%E4%B8%AD%E5%A6%82%E4%BD%95%E6%8E%A7%E5%88%B6%E9%A6%96%E9%A1%B5-%E5%BD%92%E6%A1%A3%E9%A1%B5-tag%E9%A1%B5%E4%B8%AD%E6%98%BE%E7%A4%BA%E7%9A%84%E6%96%87%E7%AB%A0%E6%95%B0/</p><p>Hexo Fluid 主題使用指南https://s81679.github.io/2020/02/25/hexo-theme-fluid/</p><p>hexo new "name" # 新建文章，会在source下的_post里新建md文件 hexo newpage "name" #新建页面，会在source下新建一个文件夹并在其中新建md文件，暂时不知道用处hexo g # 生成页面 hexo d # 部署 hexo g -d # 生成页面并部署 hexo s #本地预览 hexo clean # 清除缓存和已生成的静态文件 hexo help # 帮助</p>]]></content>
    
    
    <categories>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Hexo</tag>
      
      <tag>Fluid</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo中插入图片和注释语句的方法</title>
    <link href="/01image-insert/"/>
    <url>/01image-insert/</url>
    
    <content type="html"><![CDATA[<h1 id="图片插入的方法">图片插入的方法</h1><h2 id="方法一不好用">方法一：不好用</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 方法一">&#123;% asset_img lic.png 给图片添加的注释文字%&#125;<br></code></pre></td></tr></table></figure><p><br/><img src="/01image-insert/lic.png" class="" title="图片引用方法一"></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 方法一">&#123;% asset_img lic.png 图片长度 图片宽度 给图片添加的注释文字%&#125;<br></code></pre></td></tr></table></figure><p><br/><img src="/01image-insert/lic.png" class="" width="400" height="100" title="图片引用方法一 图片长度&#x3D;400 图片宽度&#x3D;100"></p><h2 id="方法二">方法二</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs 方法二">&lt;center&gt; # 图片不居中故采用此方法<br>&lt;img src=&quot;/images/lic.png&quot; width=400 height=250 align=&quot;middle&quot; &gt; # 根目录在source<br>&lt;br/&gt;&#123;% link hexo官网 https://hexo.io/zh-cn/ %&#125; # 超链接<br>&lt;/center&gt;<br># 或者采用如下方法居中<br>&lt;div align=center&gt;&lt;img src=&quot;/images/lic.png&quot; width=400 height=250&gt;&lt;/div&gt;<br></code></pre></td></tr></table></figure><center><img src="/images/lic.png" width=200 height=200 align="middle" ><br/><a href="https://hexo.io/zh-cn/" title="" target="">hexo官网</a></center><div data-align="center"><p><img src="/images/lic.png" width=400 height=250><br/><a href="https://hexo.io/zh-cn/" title="" target="">hexo官网</a></p></div><h2 id="方法三">方法三</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs 方法三">![图片引用方法三](/images/lic.png)<br></code></pre></td></tr></table></figure><figure><img src="/images/lic.png" alt="图片引用方法三" /><figcaption aria-hidden="true">图片引用方法三</figcaption></figure><h1 id="注释方法">注释方法</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs 注释方法"># 方法一: 使用 HTML 样式实现隐藏<br>&lt;div style=&quot;display:none&quot;&gt;<br>&lt;img src=&quot;/images/lic.png&quot; &gt;<br>&lt;/div&gt;<br><br># 方法二: 使用原生 HTML 注释语法<br>&lt;!-- <br>&lt;img src=&quot;/images/lic.png&quot; &gt; <br>&lt;img src=&quot;/images/lic.png&quot; &gt;<br>--&gt;<br><br># 方法三: 通过 Markdown 自身的解析功能，容易出问题<br>[//]: &lt;img src=&quot;/images/lic.png&quot; &gt;<br></code></pre></td></tr></table></figure><div style="display:none"><p><img src="/images/lic.png" ></p></div><!-- <img src="/images/lic.png" > <img src="/images/lic.png" >--><script src="https://utteranc.es/client.js"        repo="Antonalia/antonalia.github.io"        issue-term="pathname"        label="comment"        theme="github-light"        crossorigin="anonymous"        async></script>]]></content>
    
    
    <categories>
      
      <category>博客</category>
      
    </categories>
    
    
    <tags>
      
      <tag>写作</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>初来乍到</title>
    <link href="/hello-world/"/>
    <url>/hello-world/</url>
    
    <content type="html"><![CDATA[<center>哈哈，很高兴见到你，你好吗？<sup id="fnref:1" class="footnote-ref"><a href="#fn:1" rel="footnote"><spanclass="hint--top hint--rounded"aria-label="引用自我自己">[1]</span></a></sup></center><script src="https://utteranc.es/client.js"        repo="Antonalia/antonalia.github.io"        issue-term="pathname"        label="comment"        theme="github-light"        crossorigin="anonymous"        async></script><section class="footnotes"><div class="footnote-list"><ol><li><span id="fn:1" class="footnote-text"><span>引用自我自己<a href="#fnref:1" rev="footnote" class="footnote-backref">↩︎</a></span></span></li></ol></div></section>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>随笔</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
